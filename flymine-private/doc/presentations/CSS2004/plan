:set showbreak=\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ >
:set ts=12
:set noexpandtab
		Advanced techniques for Object Warehousing:
		Multiple inheritance, collections & references, & advanced queries
		Schema - underlying database format, cglib
		Transaction processing
		Query optimiser
		Inserting the data:
		    DBConverter & DataTranslator (and owl)
		    DataLoader:
		        Primary keys
		        Dealing with different relation types
		        Data source priorities
		
		Plan:
		Introduction:
Title	1:00	    Who I am, and what I do
Intro 1	0:40   1:40	    Caveats for this talk - ie what this talk does not do:
Intro 1		        OJB, Hibernate, etc.
Intro 1		    Warning: this talk has InterMine & Flymine as its base
		        Say that InterMine is an object-oriented data warehousing system, including an objectstore built for flexibility and performance, with powerful data integration tools.
		        Say that I will be describing things in the context of the InterMine system, but the techniques are portable to other systems.
Intro 2	1:00   2:40	    Assumptions that this talk makes:
Intro 2		        You don't have a tyrannical DBA breathing down your neck and changing the DB schema under your feet
Intro 2		        You want to build a database from data sources and present it to the data users as a read-only database (only half of the talk assumes this)
Intro 2		        You wish to use complicated data
Intro 3	1:10   3:50	    What I will be covering:
Intro 3		        Multiple inheritance and CGLib
Intro 3		        Collections and references
Intro 3		        Objectstores - brief introduction
Intro 3		        Queries and results
Intro 3		        ObjectStoreWriter and transaction processing
Intro 3		        Inserting the data:
Intro 3		            Models, ontologies, and OWL
Intro 3		            Data transformations and why a simple pipe won't work
Intro 3		            DBConverter and DataTranslator
Intro 3		            DataLoader
		Multiple inheritance and CGLib
MI 1, 2, 3	0:55   4:45	    Motivation - I'm using biological data, and it doesn't fit
MI 4	0:45   5:30	    Interfaces in the model that can have direct implementations
MI 5	0:30   6:00	    Classes in the model that implement multiple interfaces
MI 5		    Objects in the database that are no particular class from the model, but dynamically implement and extend various classes and interfaces from the model
MI 6	0:30   6:30	    Objects that implement no other interface than InterMineObject
MI 7	1:15   7:45	    What the model describes
MI 8	1:10   8:55	    Using CGLib to create objects of a dynamic class
MI 9	0:45   9:40	    Characteristics of such dynamic objects
		Schemas
DS 1	0:50  10:30	    Map all classes onto one table (cheap read & write, really bad data density affects performance, plus field type conflicts)
DS 2	0:50  11:20	    Map every class onto an individual table, put each object into only one table (cheap write, read must use UNIONs)
DS 3	0:40  12:00	    Map every class onto an individual table, with only the new fields stored in the table, and write each object into all applicable tables (write must use multiple tables, read must use JOIN)
DS 4	1:10  13:10	    Map every class onto an individual table, put each object into all applicable tables, have an OBJECT field (cheap & simple reads, write must use multiple tables, quite bad data density, but that does not affect read performance)
DS 5	1:30  14:40	    Extension to that mapping: truncated classes
Members 1	1:20  16:00	Collections and references
Members 1,2	0:45  16:45	    Proxies
		ObjectStores
OS 1	1:35  18:10	    Take queries and return results
OS 1		    Provides lookup by ID
OS 1		    Different implementations for different tasks
OS 1		        ObjectStoreInterMineImpl
OS 1		        ObjectStorePassThruImpl
--------		            ObjectStoreFastCollectionsForTranslatorImpl
--------		            ObjectStoreFastCollectionsImpl
--------		            ObjectStoreItemPathFollowingImpl
--------		            ObjectStoreSafeImpl
--------		        ObjectStoreTranslatingImpl
OS 1		        ObjectStoreClient (with ObjectStoreServer)
OS 2	1:00  19:10	    Caches
		Queries and results
Q&R 1	1:00  20:10	    Queries are effectively a description of the results
Q&R 2	1:10  21:20	    Results are Collections of rows 
Q&R 2		    Results can be multi-column, where each column can be an object, field, constant, or computed value
Q&R 3	1:00  22:20	    Results could be single-column. You could create a special single-column results Collection that is a collection of objects rather than a collection of rows, and use that as a Collection proxy
Q&R 3		    Results are loaded on-demand
Q&R 4	1:10  23:30	    Creating results is done with a query and objectstore, but doesn't actually run a query until it is accessed.
Q&R 4		    Batching reads, so the Results object is even more lazy. PrefetchManager
Q&R 5	1:00  24:30	    SQL generation and results parsing
Q&R 6	1:00  25:30	    Maintaining consistency of results by imposing ORDER BY onto the SQL
Q&R 7	1:00  26:30	    Changing the Query - don't. Or use ObjectStoreSafeImpl
		ObjectStoreWriter and transaction processing
OSW 1, 2	0:40  27:10	    Attached to the ObjectStore
OSW 2	1:05  28:15	    All operations rely on ID
OSW 2		    Delete
OSW 3	1:00  29:15	    Store:
OSW 3		        Setting IDs in the objects and related objects
OSW 3		        Deleting any previous object - and the limitations
OSW 3		        Merging collections, overwriting everything else
OSW 4	0:40  29:55	    Transactions:
OSW 4		        Beginning
OSW 4		        The state of the ObjectStore during a transaction
OSW 4		        Ending (and flushing caches)
Mot 1	1:00  30:55	Motivation for doing data integration
		Inserting the data:
		    Models, ontologies, and OWL
DL 1, 2, 3	4:00  34:55	        OWL as a format for describing models
		            Say that we are only using OWL as a description of a model and a merge specification, not as a system for reasoning. This is a slightly unusual use of OWL, and we might possibly replace it with a custom XML description instead.
DL 4, 5	2:15  37:10	        Merging models
		    Data transformations and why a simple pipe won't work
DL 6	0:35  37:45	        Three steps needed.
DL 7	0:50  38:35	        DBConverter stage: must transform M:N collections into ReferenceLists (searching in the source)
DL 8	1:00  39:35	        DataTranslator stage: must follow references in the source to attach attributes to the correct object, also may generate new objects which must be kept in memory until the end, so that they can be searched like the source
DL 9	1:10  40:45	        DataLoader stage: must be able to search in the source to build object graphs, and search in the destination to find equivalent objects
DL 10	1:10  41:55	            Searching in the source to build an object graph, using the ObjectStoreTranslatingImpl
DL 11, 12	2:50  44:45	            Searching in the destination for any equivalent objects
		                Stress the priority bit - say that some data sources may be more accurate than others for particular fields.
DL 13,14,15	2:30  47:15	            Merging together equivalent objects using information from the data tracker, and data source priorities
DL 16	1:30  48:45	        Recursion and skeletons.
		    Relationship types
DL 17	0:30  49:15	        Attributes
DL 18	0:35  49:50	        1:1 - must be consistent
DL 19	0:20  50:10	        1:N - can be ignored by the data loader, as it is never unidirectional
DL 20	0:50  51:00	        N:1 - very simple - could be unidirectional
DL 21	0:45  51:45	        M:N - handled by indirection table
		QueryOptimiser
Opt 1	0:40  52:25	    Motivation
		        Say that the optimiser we wrote can be used in other Java projects - it is not specific to InterMine.
		        Say that the reason we have brought all the data in-house is to improve the performance of cross-reference queries. Federated databases are the "any other way" that is too slow.
Opt 2	0:50  53:15	    SQL Views
Opt 3	0:45  54:00	    Precomputed table is the opposite of a view
Opt 4	0:50  54:50	    A trivial example
Opt 5	0:50  55:40	    Another example
Opt 6	1:10  56:50	    Precomputed tables are fitted into queries
Opt 7	1:20  58:10	    Optimiser asks server to EXPLAIN queries
Opt 7		    Optimiser gives up after a while
Opt 7		    Improvements between 10% and 10000% (mention machine learning problem)
Opt 8	1:00  59:10	    Decouples the model from the schema
		Future stuff - relationship metadata
Fut 1	1:00  60:10	    Motivation
Fut 2	0:40  60:50	    Diagrams
Fut 3	1:00  61:50	    Changes to objects
		Future stuff - graph reasoning
Fut 4	1:30  63:20	    What is possible
Fut 5	1:20  64:40	    What we're doing about it


		Improving performance in Object Warehousing:
		The N + 1 reads problem
		Large offsets
		Batching writes & using COPY etc. Avoiding deletes by caching generated IDs. Avoiding waiting for the connection in getSerial
		Three-thread pipeline:
		    Batching & ReadAheadDBReader
		    Objectstore prefetch on ItemReader
		    sql.writebatch.Batch writer thread
		    DataTracker write-back cache & thread
		    Multiprocessor systems
		DataLoader:
		    ID map
		    Cheating in getEquivalentObjects and returning a ProxyReference
		    Batching writes by having getEquivalentObjects use osw.getObjectStore
		    Possibility for parallelising getEquivalentObjects (depends on whether disk-seek-bound)
		Remove any unneccessary features (InterMineObject table, OBJECT field)
		
		Plan:
		Introduction:
Title	1:00	    Who I am, and what I do
Intro 1	0:40   1:40	    Warning: this talk has InterMine & FlyMine as its base
Intro 1		    Go to the previous talk, although it is not vital
Intro 2	0:20   2:00	    What I will be covering:
Intro 2		        Large offsets in SQL
Intro 2		        Writing to the database
Intro 2		        Reading from the database (N + 1 reads etc)
Intro 2		        The three-thread pipeline
Intro 2		        Database-backed cached table (DataTracker)
Intro 2		        Multiprocessor systems
Intro 2		        Speeding up the DataLoader
Intro 2		        Other tips
LO, LO 2	1:45   3:45	Large offsets (using an index on the sort key)
		How to do sorting by multiple columns properly
Opt 1	1:00   4:45	    The problem
Opt 2	1:20   6:05	    What the database server can do about it (but doesn't)
Opt 3	1:40   7:45	    A workaround, using the optimiser
Round-trip	2:00   9:45	The principle of round-trip-time (make accesses big)
		Writing to the database
Write 1	0:45  10:30	    The store method needs unique IDs, get them efficiently
Write 2	1:00  11:30	    When you write and you are in a transaction, batch together the writes and commit later
Write 2		    Use the fastest insert method available to you - eg COPY
Write 3	1:00  12:30	    Perform writes in another thread - that means your data-producing thread can keep on working while the database is writing your data
Write 3		    Really do as little processing as possible in the writing thread - perfect scenario is to completely prepare the statements in the producing thread, so the writing thread is only ever waiting
Write 3		    Make sure that we still properly flush the write queue whenever there is a commit or a read
Write 4	3:00  15:30	    Diagram of org.intermine.sql.writebatch.
Write 5	3:30  19:00	    Prototypes of BatchWriter, FlushJob, and Batch.
Write 6	0:50  19:50	    Make sure you really do have multi-threaded action, and aren't blocking the producing thread
Write 6		    Make sure there aren't any other database accesses that the producing thread makes that would cause the producing thread to wait for the writing thread (eg ID production)
Write 7	1:20  21:10	    If flushes will happen very often, try and reduce their weight:
		        Allow partial flushes.
		        Don't bother passing across empty jobs, but wait for unfinished activity and pass back exceptions.
Write 8	0:40  21:50	    Deleting rows is expensive - if you are merely filling a database, you should be able to avoid them completely, then remove indexes. So, cache which IDs you have generated but not stored, and don't bother deleting those.
		Reading from the database
		    Batching reads
Read 1	1:00  22:50	        Choose a sensible batch size - bigger is faster, but only to a point, and there is memory to consider
		    The N + 1 reads problem:
Read 2	0:40  23:30	        Why it is a problem
Read 3	1:00  24:10	        You could rewrite your queries, so that another column is the content of a reference - BUT this breaks unless you use LEFT OUTER JOIN, and doesn't work very well for collections
Read 3		        You could link the proxies together somehow, so they would trigger a batch read when one is accessed
Read 4	1:30  26:00	        You could explicitly use an ObjectStore that does it for you - like ObjectStoreFastCollectionsImpl
Read 4		        You may even put explicit knowledge of what data will be needed into the batching system, like ObjectStoreFastCollectionsForTranslatorImpl (for the DataLoader), and ObjectStoreItemPathFollowingImpl (for the DataTranslator)
Read 4		        In the BatchingDBReader, we build a cache of likely SQL queries with results brought in by the batch
		    Prefetching data in another thread
Read 5	1:30  27:30	        This means that when your consuming thread is ready for more data, it is immediately available
Read 5	1:00  28:30	        Do as little processing in the producing thread as possible - perfect scenario is for this thread to ONLY wait for the database or the consuming thread
Read 5		        You can guess what data will be required next, as with the PrefetchManager
Read 5		        You can know what data will be required next, as with the DBReader
		        In general, if you are performing a bulk data transfer, prefetch will help a lot
		The three-thread pipeline
Three	0:50  29:20	    The ideal single-processor bulk transfer system
Three		    Ideally, the processing thread never waits for either the reading thread or the writing thread, therefore it is running at the fastest possible speed
Three		    Broken if you need to search in the destination database.
		Multiprocessor systems
SMP 1	1:10  30:30	    There is a large overhead involved in moving data from one processor to another - so don't (eg. switching on Hyperthreading with the above system reduced performance by 40%)
SMP 1		        Java is particularly bad at this.
SMP 1		        An array of ints is faster than an array of Integers. Tru64: 17, 65. Athlon: 33, 619. Dual xeon: 15, 411
		            Random order: Athlon: 143, 839. Tru64: 23, 155. Dual Xeon: 77, 486
SMP 1		    Guage the ratio of data quantity to processing time. If your program spends lots of time on small amounts of data, then multiprocessor systems will work a lot better than little processing on large amounts of data
SMP 2	1:00  31:30	    Particularly, don't do a pipeline like this: Reader -> Process 1 -> Process 2 -> Process 3 -> Writer
SMP 2		    If your program does a reasonable amount of processing on a non-huge amount of data, then you could get away with:
		                               .-> Process 1 -.
		                       Reader ---> Process 2 ---> Writer
		                               `-> Process 3 -'
SMP 3	0:45  32:15	    If you are processing lots of data, then something like this would be better:
		                       Reader 1 -> Process 1 -> Writer 1
		                       Reader 2 -> Process 2 -> Writer 2
		                       Reader 3 -> Process 3 -> Writer 3
		        However, in this case you will really want each thread group to be on a single processor, or you get data transferred between processors again. (difficult anyway)
SMP 4	1:00  33:15	    To solve that problem, you will need to write one thread per processor, but try and use java.nio in order to get non-blocking reads -> difficult with JDBC, but fine with files or sockets
SMP 5	1:20  34:35	    However, your program may be IO bound anyway - throwing more CPU at it won't make it faster - make sure you engineer out the bottlenecks, rather than the fast bit
SMP 5		    Additionally, applying multiple reader/writer threads to a disk usually will not help
SMP 5		    This is a difficult topic, still subject to research, so try whatever you think of. If it goes faster with your idea, you weren't wrong
SMP 6	1:20  35:55	    To make this work properly, you need to learn all about CPUs, caches, memory latency, etc.
SMP 6		    If you can split the job between multiple machines easily, then that will work well.
		A database-backed lookup table (example - our DataTracker)
		    Different paradigm to the bulk transfer system
Track 1	1:20  37:15	        There is random access.
Track 1		        There is too much data to fit into memory, so it must overflow into a database table.
Track 1		        Need cache.
Track 1		        Want to use bulk writes to the database, because they are faster.
Track 1		        We still want to never wait for the writer.
		    We want to construct a LRU cache, and always flush old entries out to the database
Track 2	1:30  38:45	        LinkedHashMap is good for this - but you need a maximum size
Track 2		    Writes always go to the cache
Track 2		    When it has become too big, atomically move a certain number into a different map
Track 2		    Write this lot to the DB in a separate thread/connection
Track 3	1:30  40:15	    The reader must be able to read this second map until after the data is committed to the database, or entries will go missing while they are being written
Track 3		    When the flush is finished, the second map is atomically removed, freeing memory for new entries.
Track 3		    If entries are created faster than they can be written, you must block the user.
Track 4	2:00  42:15	    Picture.
		Speeding up the DataLoader
DL 1	1:45  44:00	    The DataLoader is more tricky than the other two stages to speed up.
DL 2, DL 3	2:00  46:00	        The overwhelming bottleneck is the getEquivalentObjects() query
DL 3		        This query searches for equivalent objects in the destination database. For each object to be stored, all connected objects need to be matched up against their equivalent objects in the destination in order to store the object with its collections and references properly
		    The ID Map
DL 4	1:00  47:00	        However, we only need to getEquivalentObjects() for each object once
DL 4		        So, we make an assumption: if we see an object for the second time, we can recognise it by ID, and return the same results as last time
DL 4		        In fact, the last time we saw that object, we merged ALL the equivalent objects together into one object in the destination database
DL 5	1:20  48:20	        Therefore there is a mapping between source object and destination object, indexed by ID, and we can create an efficient ID to ID map for this purpose
DL 6	1:30  49:50	        This converts a complex query into a getObjectById(), which may hit the cache.
DL 6		        This also simplifies the getEquivalentObjects query, as it may not need to recurse down primary key references
		    Cheating, and avoiding getObjectById
DL 7	1:20  51:10	        If an object has been previously seen, it is unlikely that it will be actually needed, so one can put in a ProxyReference instead. We can reify the ProxyReference if we need it
DL 7		        Handily, ProxyReference is an InterMineObject, so when an object pointing to it is stored, it is stored with a correct pointer to the existing object with that ID in the database
		    GetEquivalentObjects calls can be made long before the data is needed
DL 8	1:00  52:10	        Consider the circumstance where an object is presented to be stored twice. The first time getEquivalentObjects runs a search, and the second time it returns a ProxyReference from the ID map. Consider also that the results of a getEquivalentObject query cannot be changed by any action other than that of storing an object equivalent to the one we are interested in.
DL 8		        If we can make the assumption: we will never attempt to store an object that is equivalent to a previously-stored object (after some point in time) that the ID map does not recognise we can then allow the getEquivalentObjects call to use "old" data, as old as that point in time.
-----------		        This assumption is false for objects that have a null ID, because the ID map does not recognise them, so this cannot be applied to such objects
		        This allows a performance boost for several reasons:
DL 9	1:00  53:10	            Reads can be done over a separate connection outside the transaction (old data...), therefore allowing writes to batch up in the ObjectStoreWriter - not much advantage as the bottleneck is the rate at which getEquivalentObjects queries can be answered
DL 10	0:40  53:40	            We can parallelise this getEquivalentObjects data collection by using several connections - again not much advantage as this does not reduce the amount of work that the database server has to do
DL 11	1:30  54:20	            We can batch the queries for the data collection - this probably will improve performance, as it allows the database to use more efficient algorithms with less overhead in order to do many times more work per query in proportionally less time.
DL 12	2:00  57:20	        Knowing when to stop
		Other tips:
Other 1	1:30  58:50	    Get the database indexes right
Other 1		    Profile your program, and work out where it is spending its time, then optimise that bit
Other 1		    Use StringBuffer, rather than concatenating Strings
Other 1		    Remove any unneccessary features that slow down the system - eg the InterMineObject table and OBJECT field
Other 2	1:40  60:30	    Throw a bigger computer at it
Other 2		    Beware round-trip times (again)
Other 3	2:00  62:30	    SQL_ASCII rather than UNICODE
Other 3		    Moore's law:
